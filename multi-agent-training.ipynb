{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a multi-agent with LangGraph ü¶úüï∏Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we're going to build a multi-agent system that can handle complex customer support queries for a digital music store. We'll be covering core LangGraph/LangGraph Platform concepts such as human-in-the-loop, the remote graph interface, and short/long-term memory.\n",
    "\n",
    "![Architecture](images/architecture.png)\n",
    "\n",
    "Our agent is connected to the [Chinook database](https://www.sqlitetutorial.net/sqlite-sample-database/), and is able to handle queries related to customer information, invoices, and the products offered by the store. **Our multi-agent system will utilize 3 sub-agents that have been built and deployed** by our teammates: \n",
    "1. Customer information subagent ‚Äì Manages customer account details (name, address, phone, email).\n",
    "2. Music catalog information subagent ‚Äì Provides information about the digital music store‚Äôs catalog (albums, tracks, songs).\n",
    "3. Invoice information subagent ‚Äì Retrieves a customer‚Äôs past purchases or invoices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Building the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install python-dotenv langgraph typing-extensions langchain-core langchain-openai langchain-anthropic langchain-community scikit-learn openai ipython openevals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the environment variables from our .env file and define an LLM that'll power our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "# Alternatively, you can use AzureChatOpenAI()\n",
    "# model = AzureChatOpenAI()\n",
    "model = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State\n",
    "\n",
    "First, we'll initialize **state**, which is the short term memory of our agent. At a low level, our agent's state is the input schema that‚Äôs shared across all the the nodes and edges in our graph. Our state will track the following fields: \n",
    "1. The user's initial request\n",
    "2. The agent's action plan\n",
    "3. What steps the agent has executed so far\n",
    "4. Conversation history \n",
    "5. Final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import Annotated, List, Tuple, Literal, Union\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "import operator\n",
    "\n",
    "class State(TypedDict):\n",
    "    original_objective: str\n",
    "    action_plan: List[str]\n",
    "    past_steps: Annotated[List[Tuple], operator.add]\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define a system prompt for our agent that we'll reuse throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor_reusable_context = \"\"\"You are an expert customer support assistant for a digital music store. \n",
    "You are dedicated to providing exceptional service and ensuring customer queries are answered thoroughly. \n",
    "You have a team of subagents that you can use to help answer queries from customers. \n",
    "Your primary role is to serve as a supervisor/planner for this multi-agent team that helps answer queries from customers. \n",
    "The multi-agent team you are supervising is responsible for handling questions related to the digital music store's music \n",
    "catalog (albums, tracks, songs, etc.), information about a customer's account (name, email, phone number, address, etc.), \n",
    "and information about a customer's past purchases or invoices. Your team is composed of three subagents that you can use \n",
    "to help answer the customer's request:\n",
    "\n",
    "1. customer_information_subagent: this subagent is able to retrieve and update the personal information associated with \n",
    "a customer's account in the database (specifically, viewing or updating a customer's name, address, phone number, or email).\n",
    "2. music_catalog_information_subagent: this subagent is able to retrieve information about the digital music store's music \n",
    "catalog (albums, tracks, songs, etc.) from the database.\n",
    "3. invoice_information_subagent: this subagent is able to retrieve information about a customer's past purchases or invoices \n",
    "from the database. \\n\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing Short-term & Long-term Memory\n",
    "**Long term memory** stores information that is persisted across threads. Let's first define the long term memory store where we will be saving our user's music preferences.\n",
    "\n",
    "We will also initialize a checkpointer for **short-term memory**. Short-term memory is persisted across a single thread. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# Initializing long term memory store \n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "# Initializing checkpoint for thread-level memory \n",
    "checkpointer = MemorySaver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 1: Supervisor\n",
    "Now that we've initialized our agent's short and long term memory, we're ready to start building our first node.\n",
    "\n",
    "Our first node is responsible for taking in a customer's query and generating an action plan to answer it. This action plan will include the steps we need to execute and the subagent that should handle each step. Our supervisor is able to use the agent's long term memory to personalize the plan to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Step(BaseModel):\n",
    "    description: str = Field(description=\"Description of the step to be performed\")\n",
    "    subagent: Literal[\"customer_information_subagent\", \"music_catalog_information_subagent\", \"invoice_information_subagent\"] = Field(\n",
    "        description=\"Name of the subagent that should execute this step\"\n",
    "    )\n",
    "    \n",
    "class Plan(BaseModel):\n",
    "    steps: List[Step] = Field(\n",
    "        description=\"Different steps that the subagents should follow to answer the customer's request, in chronological order\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.store.base import BaseStore\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "supervisor_prompt = supervisor_reusable_context + \"\"\"\n",
    "Your role is to create an action plan that the subagents can follow and execute to thoroughly answer the customer's request. \n",
    "Your action plan should specify the exact steps that should be followed by the subagent(s) in order to successfully answer \n",
    "the customer's request, and which subagent should perform each step. Return the action plan as a list of objects, where each \n",
    "object contains the following fields:\n",
    "- step: a detailed step that should be followed by subagent(s) in order to successfully answer a portion of the customer's request.\n",
    "- subagent_to_perform_step: the subagent that should perform the step.\n",
    "\n",
    "Return the action plan in chronological order, starting with the first step to be performed, and ending with the last step \n",
    "to be performed. You should try your best not to have multiple steps performed by the same subagent. This is inefficient. \n",
    "If you need a subagent to perform a task, try and group it all into one step.\n",
    "\n",
    "You have existing information/long term memory about the customer that you can use to help you create the action plan. \n",
    "The existing information about the customer is as follows:\n",
    "{existing_customer_information}\n",
    "\n",
    "If you do not need a subagent to answer the customer's request, do not include it in the action plan/list of steps. Your goal \n",
    "is to be as efficient as possible, while ensuring that the customer's request is answered thoroughly. Take a deep breath and \n",
    "think carefully before responding. Go forth and make the customer delighted!\n",
    "\"\"\"\n",
    "\n",
    "# helper \n",
    "def format_user_memory(user_data):\n",
    "    \"\"\"Fetches music preferences from users, if available.\"\"\"\n",
    "    profile = user_data['memory']\n",
    "    result = \"\"\n",
    "    if hasattr(profile, 'music_preferences') and profile.music_preferences:\n",
    "        result += f\"Music Preferences: {', '.join(profile.music_preferences)}\"\n",
    "    return result.strip()\n",
    "\n",
    "\n",
    "# helper \n",
    "def format_action_plan(steps_list):\n",
    "    \"\"\"\n",
    "    Process a list of Step objects and format them into a comprehensive summary string.\n",
    "    \n",
    "    Args:\n",
    "        steps_list: List of Step objects with description and subagent attributes\n",
    "        \n",
    "    Returns:\n",
    "        Formatted string with key information about agent actions\n",
    "    \"\"\"\n",
    "    summary = \"# ACTION PLAN: \\n\\n\"\n",
    "    \n",
    "    for i, step in enumerate(steps_list, 1):\n",
    "        # Extract information from Step object\n",
    "        task_description = step.description\n",
    "        subagent = step.subagent\n",
    "        \n",
    "        # Format the action summary\n",
    "        summary += f\"## Action {i}\\n\\n\"\n",
    "        summary += f\"### Agent\\n{subagent}\\n\\n\"\n",
    "        summary += f\"### Task\\n{task_description}\\n\\n\"\n",
    "        \n",
    "        # Add separator between actions except for the last one\n",
    "        if i < len(steps_list):\n",
    "            summary += \"---\\n\\n\"\n",
    "    return summary\n",
    "\n",
    "\n",
    "\n",
    "# Node \n",
    "def supervisor(state: State, config: RunnableConfig, store: BaseStore) -> dict:\n",
    "    \"\"\"Fetches relevant memory profiles and returns an plan\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50 + \"üéØ SUPERVISOR FUNCTION CALLED\" + \"=\"*50)\n",
    "    \n",
    "    # Fetch existing user memory from long term memory store \n",
    "    user_id = config[\"configurable\"].get(\"user_id\")\n",
    "    namespace = (\"memory_profile\", user_id)\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "    formatted_memory = \"\"\n",
    "    if existing_memory and existing_memory.value:\n",
    "        formatted_memory = format_user_memory(existing_memory.value)\n",
    "\n",
    "    # Fetch user input  \n",
    "    first_message = state[\"messages\"][-1]\n",
    "\n",
    "    # Enforce structured output from LLM \n",
    "    structured_model = model.with_structured_output(Plan)\n",
    "    result = structured_model.invoke([\n",
    "        SystemMessage(content=supervisor_prompt.format(existing_customer_information=formatted_memory))\n",
    "    ] + [first_message])\n",
    "\n",
    "    formatted_action_plan = \"Based on the plan, there is no need to route to sub-agents.\" \n",
    "    \n",
    "    if result.steps: \n",
    "        formatted_action_plan = format_action_plan(result.steps)\n",
    "\n",
    "    print(\"System Message: \" + formatted_action_plan)\n",
    "    \n",
    "    return {\n",
    "        # Update State\n",
    "        \"action_plan\": result.steps,\n",
    "        \"original_objective\": first_message.content,\n",
    "        # Streaming intermediatery inputs\n",
    "        \"messages\": [SystemMessage(content=formatted_action_plan)]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 2: User input\n",
    "In the second node, we are introducing **human-in-the-loop**. This will allow the customer to give feedback on the action plan. We will take in the user's feedback and update the action plan accordingly. If there is no feedback, we won't change the initial plan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanWithUserInput(BaseModel):\n",
    "    steps: List[Step] = Field(\n",
    "        description=\"Different steps that the subagents should follow to answer the customer's request, in chronological order\"\n",
    "    )\n",
    "    updated_objective: str = Field(\n",
    "        description=\"The updated objective/request from the customer, after taking into account the user's input/ideas for improvement\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import interrupt, Command\n",
    "\n",
    "# Prompt \n",
    "\n",
    "replan_with_user_input_prompt = supervisor_reusable_context + \"\"\"\n",
    "Before you, another supervisor/planner has already created an action plan for the subagents to follow. \n",
    "This action plan was then shown to the customer so they could provide feedback and ideas for improvement. \n",
    "\n",
    "Your job is to take the customer's feedback and update the action plan and their original request/objective \n",
    "accordingly. Below, I have attached the previous action plan, the customer's original request/objective that \n",
    "the action plan was created for, and the feedback/ideas for improvement from the customer.\n",
    "\n",
    "Please take this feedback and construct a new action plan/original objective that the subagents can follow to \n",
    "thoroughly answer the customer's request. If the feedback is not relevant or nonsensical, you can ignore it, \n",
    "and keep the original action plan/original objective.\n",
    "\n",
    "The updated action plan should specify the exact steps that should be followed by the subagent(s) in order to \n",
    "successfully answer the customer's request, and which subagent should perform each step. Return the action plan \n",
    "as a list of objects, where each object contains the following fields:\n",
    "- step: a detailed step that should be followed by the subagent(s) in order to successfully answer a portion of \n",
    "the customer's request.\n",
    "- subagent_to_perform_step: the subagent that should perform the step.\n",
    "\n",
    "The updated objective should be a detailed description of the customer's request/objective, after taking into \n",
    "account the customer's feedback/ideas for improvement.\n",
    "\n",
    "Return the action plan in chronological order, starting with the first step to be performed, and ending with \n",
    "the last step to be performed. You should try your best not to have multiple steps performed by the same subagent. \n",
    "This is inefficient. If you need a subagent to perform a task, try and group it all into one step.\n",
    "\n",
    "If you do not need a subagent to answer the customer's request, do not include it in the action plan/list of steps. \n",
    "Your goal is to be as efficient as possible, while ensuring that the customer's request is answered thoroughly and \n",
    "to the best of your ability.\n",
    "\n",
    "Use the information below to update the action plan and customer's objective based on their feedback/ideas for improvement.\n",
    "\n",
    "*IMPORTANT INFORMATION BELOW*\n",
    "\n",
    "Your original objective/request from the customer was this:\n",
    "{original_objective}\n",
    "\n",
    "The original action plan constructed by the previous supervisor/planner was this:\n",
    "{formatted_action_plan}\n",
    "\n",
    "The customer's feedback/ideas for improvement are as follows:\n",
    "{user_input}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Node \n",
    "def human_input(state: State, config: RunnableConfig, store: BaseStore) -> dict:\n",
    "    print(\"\\n\" + \"=\"*50 + \"üë§ HUMAN INPUT FUNCTION CALLED\" + \"=\"*50)\n",
    "    \n",
    "    user_input = interrupt({\"Task\": \"Review the generated plan and suggest any revisions.\"})\n",
    "    \n",
    "    original_objective = state[\"original_objective\"]\n",
    "    formatted_action_plan = format_action_plan(state[\"action_plan\"])\n",
    "    \n",
    "    # structure model output \n",
    "    structured_model = model.with_structured_output(PlanWithUserInput)\n",
    "    \n",
    "    if user_input != \"\": \n",
    "        result = structured_model.invoke([SystemMessage(content=replan_with_user_input_prompt.format(original_objective=original_objective, formatted_action_plan=formatted_action_plan, user_input=user_input))])\n",
    "        updated_action_plan = \"Updated action plan based on user proposal: \\n\" + format_action_plan(result.steps)\n",
    "        print(updated_action_plan)\n",
    "        return {\n",
    "            # update state \n",
    "            \"action_plan\": result.steps,\n",
    "            \"original_objective\": result.updated_objective,\n",
    "            \"messages\": [SystemMessage(content=updated_action_plan)]\n",
    "        }\n",
    "    else:\n",
    "        update_msg = \"No updates from user input. Will proceed with the original plan.\"\n",
    "        print(\"System Message: \" + update_msg)\n",
    "        return{\n",
    "            \"messages\": [SystemMessage(content=update_msg)]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 3. Agent executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **RemoteGraph** interface from LangGraph Platform enables seamless interaction with deployed agents.\n",
    "\n",
    "My teammates have built and deployed three specialized subagents:\n",
    "1. customer_information_subagent ‚Äì Manages customer account details (name, address, phone, email).\n",
    "2. music_catalog_information_subagent ‚Äì Provides information about the digital music store‚Äôs catalog (albums, tracks, songs).\n",
    "3. invoice_information_subagent ‚Äì Retrieves a customer‚Äôs past purchases or invoices.\n",
    "\n",
    "The agent_executor node calls the relevant sub-agent to execute based on the step we are processing from the action plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.pregel.remote import RemoteGraph\n",
    "customer_information_deployment_url = \"https://remote-customer-assistant-cbd474a176cb59fe87deddf6f14ce85c.us.langgraph.app\"\n",
    "music_catalog_information_deployment_url = \"https://remote-music-assistant-56295a1f54e5561f812da52437d18097.us.langgraph.app\"\n",
    "invoice_information_deployment_url = \"https://remote-invoice-assistant-986c987c38e9536ea5aa870c6082254e.us.langgraph.app\"\n",
    "\n",
    "customer_information_remote_graph = RemoteGraph(\"agent\", url=customer_information_deployment_url)\n",
    "music_catalog_information_remote_graph = RemoteGraph(\"agent\", url=music_catalog_information_deployment_url)\n",
    "invoice_information_remote_graph = RemoteGraph(\"agent\", url=invoice_information_deployment_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_executor(state: State, config: RunnableConfig, store: BaseStore) -> dict:\n",
    "    print(\"\\n\" + \"=\"*50 + \"ü§ñ AGENT EXECUTOR FUNCTION CALLED\" + \"=\"*50)\n",
    "    plan = state[\"action_plan\"]\n",
    "\n",
    "    if plan: \n",
    "        total_plan = \"\\n\".join([\n",
    "            f\"{i+1}. {step.subagent} will: {step.description}\" \n",
    "            for i, step in enumerate(plan)\n",
    "        ])\n",
    "        first_task_subagent = plan[0].subagent\n",
    "        first_task_description = plan[0].description\n",
    "        first_task_subagent_response = first_task_subagent + \" executed logic to answer the following request from the planner/supervisor: \" + first_task_description + \".\\n Please output your report on your work progress to your supervisor as a update on your action and progress, rather than a end-user facing message. \"\n",
    "        task_formatted_prompt = f\"\"\"For the following plan: \n",
    "        {total_plan}\n",
    "        You are tasked with executing the first step #1. {first_task_subagent} will: {first_task_description}\n",
    "        \"\"\"\n",
    "        # Calls the appropriate sub-agent with context\n",
    "        if first_task_subagent == \"customer_information_subagent\":\n",
    "            response = customer_information_remote_graph.invoke({\"messages\": [HumanMessage(content=task_formatted_prompt)]})[\"messages\"]\n",
    "            final_response = response[-1]['content']\n",
    "        elif first_task_subagent == \"music_catalog_information_subagent\":\n",
    "            response = music_catalog_information_remote_graph.invoke({\"messages\": [HumanMessage(content=task_formatted_prompt)]})[\"messages\"]\n",
    "            final_response = response[-1]['content']\n",
    "        elif first_task_subagent == \"invoice_information_subagent\":\n",
    "            response = invoice_information_remote_graph.invoke({\"messages\": [HumanMessage(content=task_formatted_prompt)]})[\"messages\"]\n",
    "            final_response = response[-1]['content']\n",
    "\n",
    "        system_msg = \"Executed \" + first_task_subagent + \":\\n\" + final_response\n",
    "        print(\"System Message: \" + system_msg)\n",
    "            \n",
    "        return {\n",
    "            # Update State \n",
    "            \"past_steps\": [\n",
    "                (first_task_subagent_response, final_response)\n",
    "            ],\n",
    "            \"messages\": [SystemMessage(content=system_msg)]\n",
    "    } \n",
    "    else: \n",
    "        system_msg = \"No agent executed based on current plan. Proceeding to replanner step.\"\n",
    "        print(\"System Message: \" + system_msg)\n",
    "        return {\n",
    "            \"messages\": [SystemMessage(content=system_msg)]\n",
    "    } \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Replanner\n",
    "The **replanner** node reviews the actions that have been taken by our sub-agents so far, and updates the action plan to ensure the customer's request is being answered correctly. This step helps our agent recover from failure states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response(BaseModel):\n",
    "    \"\"\"Response to user.\"\"\"\n",
    "    response: str\n",
    "\n",
    "\n",
    "class ReplannerResponse(BaseModel):\n",
    "    action: Union[Response, Plan] = Field(\n",
    "        description=\"The action to perform. If you no longer need to use the subagents to solve the customer's request, use Response. \"\n",
    "        \"If you still need to use the subagents to solve the problem, construct and return a list of steps to be performed by the subagents, use Plan.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "replanner_prompt = supervisor_reusable_context + \"\"\"Before you, another supervisor/planner has already created an action \n",
    "plan for the subagents to follow. I've attached the previous action plan, the customer's original request, and the steps \n",
    "that have already been executed by the subagents below. \n",
    "You should take this information and either update the plan, or return Response if you believe the customer's request has been answered \n",
    "thoroughly and you no longer need to use the subagents to solve the problem (including when their inquiry doesn't need any subagents).  \n",
    "\n",
    "If you decide to update the action plan, your action plan should specify the exact steps that should be followed by the \n",
    "subagent(s) in order to successfully answer the customer's request, and which subagent should perform each step. Return \n",
    "the action plan as a list of objects, where each object contains the following fields:\n",
    "- step: a detailed step that should be followed by subagent(s) in order to successfully answer a portion of the customer's request.\n",
    "- subagent_to_perform_step: the subagent that should perform the step.\n",
    "\n",
    "Return the action plan in chronological order, starting with the first step to be performed, and ending with the last step to be performed. \n",
    "You should try your best not to have multiple steps performed by the same subagent. This is inefficient. If you need a subagent to \n",
    "perform a task, try and group it all into one step.\n",
    "\n",
    "If you do not need a subagent to answer the customer's request, do not include it in the action plan/list of steps. Your goal \n",
    "is to be as efficient as possible, while ensuring that the customer's request is answered thoroughly and to the best of your ability.\n",
    "\n",
    "Use the information below to either update the action plan, or return Response if you believe the customer's request has been \n",
    "answered thoroughly and you no longer need to use the subagents to solve the problem. If you return Response, make sure it's a neatly formatted, thorough response that can be shown\n",
    "to a customer. Make sure the response has all the information the customer needs, and is professional and friendly.\n",
    "\n",
    "*IMPORTANT INFORMATION BELOW*\n",
    "\n",
    "Your original objective/request from the customer was this:\n",
    "{original_objective}\n",
    "\n",
    "The original action plan constructed by the previous supervisor/planner was this:\n",
    "{formatted_action_plan}\n",
    "\n",
    "The subagents have already executed the following steps:\n",
    "{formatted_steps}\n",
    "\n",
    "If no more steps are needed and you are ready to respond to the customer, use Response. If you still need to use the subagents to solve the problem, construct and return a list of steps to be performed by the subagents using Plan.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# helpers \n",
    "def format_steps(steps_list):\n",
    "    \"\"\"\n",
    "    Convert a list of Step objects into a neatly formatted string.\n",
    "    \n",
    "    Args:\n",
    "        steps_list: List of Step objects\n",
    "        \n",
    "    Returns:\n",
    "        A formatted string containing all key information\n",
    "    \"\"\"\n",
    "    formatted_output = \"# STEPS ALREADY PERFORMED BY THE SUBAGENTS\\n\\n\"\n",
    "    \n",
    "    for i, step in enumerate(steps_list, 1):\n",
    "        context_on_step = step[0]\n",
    "        result_of_step = step[1]\n",
    "        formatted_output += f\"## Step {i}\\n\\n\"\n",
    "        formatted_output += f\"### Task Description\\n{context_on_step}\\n\\n\"\n",
    "        formatted_output += f\"### Result\\nThe following is the response returned after the subagent performed the task above:\\n\\n\"\n",
    "        # Add the result content\n",
    "        formatted_output += f\"{result_of_step}\\n\\n\"    \n",
    "        # Add separator between steps\n",
    "        if i < len(steps_list):\n",
    "            formatted_output += \"---\\n\\n\"\n",
    "    return formatted_output\n",
    "\n",
    "\n",
    "# Node \n",
    "def replanner(state: State, config: RunnableConfig, store: BaseStore) -> dict:\n",
    "    print(\"\\n\" + \"=\"*50 + \"üîÑ REPLANNER FUNCTION CALLED\" + \"=\"*50)\n",
    "    \n",
    "    # Fetch from state \n",
    "    original_objective = state[\"original_objective\"]\n",
    "    previous_action_plan = state[\"action_plan\"]\n",
    "    previous_steps = state[\"past_steps\"]\n",
    "\n",
    "    # format action steps & action plan \n",
    "    formatted_action_plan = format_action_plan(previous_action_plan)\n",
    "    formatted_steps = format_steps(previous_steps)\n",
    "\n",
    "    # structured output with response or updated action_plan \n",
    "    structured_model = model.with_structured_output(ReplannerResponse)\n",
    "    result = structured_model.invoke([SystemMessage(content=replanner_prompt.format(original_objective = original_objective, formatted_action_plan = formatted_action_plan, formatted_steps = formatted_steps) )])\n",
    "    if isinstance(result.action, Response):\n",
    "        print(\"System Message: \" + result.action.response)\n",
    "        return {\"response\": result.action.response, \"messages\": [SystemMessage(content=result.action.response)]}\n",
    "    else:\n",
    "        formatted_update_plan = \"Action plan has been updated. \\n\" + format_action_plan(result.action.steps)\n",
    "        print(\"System Message: \" + formatted_update_plan)\n",
    "        return {\"action_plan\": result.action.steps, \"messages\": [SystemMessage(content=formatted_update_plan)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we're ready to return an answer to the user, we want to output the response, not an updated plan. To accomplish this, we'll define a **conditional edge**. Inside the conditional edge, we will:\n",
    "1. Update our agent's long term memory and route to END if we're finished executing the plan\n",
    "2. Route back to the agent_executor with the updated plan if we still need to take some action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserProfile(BaseModel):\n",
    "    customer_id: str = Field(\n",
    "        description=\"The customer ID of the customer\"\n",
    "    )\n",
    "    music_preferences: List[str] = Field(\n",
    "        description=\"The music preferences of the customer\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_memory_prompt = \"\"\"You are an expert analyst that is observing a conversation that has taken place between a customer and a customer support assistant. The customer support assistant works for a digital music store, and has utilized a multi-agent team to answer the customer's request. \n",
    "You are tasked with analyzing the conversation that has taken place between the customer and the customer support assistant, and updating the memory profile associated with the customer. The memory profile may be empty. If it's empty, you should create a new memory profile for the customer.\n",
    "\n",
    "You specifically care about saving any music interest the customer has shared about themselves, particularly their music preferences to their memory profile.\n",
    "\n",
    "To help you with this task, I have attached the conversation that has taken place between the customer and the customer support assistant below, as well as the existing memory profile associated with the customer that you should either update or create. \n",
    "\n",
    "The customer's memory profile should have the following fields:\n",
    "- customer_id: the customer ID of the customer\n",
    "- music_preferences: the music preferences of the customer\n",
    "\n",
    "These are the fields you should keep track of and update in the memory profile. If there has been no new information shared by the customer, you should not update the memory profile. It is completely okay if you do not have new information to update the memory profile with. In that case, just leave the values as they are.\n",
    "\n",
    "*IMPORTANT INFORMATION BELOW*\n",
    "\n",
    "The conversation between the customer and the customer support assistant that you should analyze is as follows:\n",
    "{conversation}\n",
    "\n",
    "The existing memory profile associated with the customer that you should either update or create based on the conversation is as follows:\n",
    "{memory_profile}\n",
    "\n",
    "Ensure your response is an object that has the following fields:\n",
    "- customer_id: the customer ID of the customer\n",
    "- music_preferences: the music preferences of the customer\n",
    "\n",
    "For each key in the object, if there is no new information, do not update the value, just keep the value that is already there. If there is new information, update the value. \n",
    "\n",
    "Take a deep breath and think carefully before responding.\n",
    "\"\"\"\n",
    "\n",
    "def should_end(state: State, config: RunnableConfig, store: BaseStore):\n",
    "    print(\"\\n\" + \"=\"*50+\"üé¨ SHOULD_END FUNCTION CALLED\"+\"=\"*50)\n",
    "    \n",
    "    past_messages = state[\"messages\"]\n",
    "    \n",
    "    if \"response\" in state and state[\"response\"]:\n",
    "        # Fetch and update memory \n",
    "        user_id = config[\"configurable\"].get(\"user_id\")\n",
    "        namespace = (\"memory_profile\", user_id)\n",
    "        existing_memory = store.get(namespace, \"user_memory\")\n",
    "        if existing_memory and existing_memory.value:\n",
    "            existing_memory_dict = existing_memory.value\n",
    "            formatted_memory = (\n",
    "                f\"Music Preferences: {', '.join(existing_memory_dict.get('music_preferences', []))}\"\n",
    "            )\n",
    "        else:\n",
    "            formatted_memory = \"\"\n",
    "        formatted_system_message = SystemMessage(content=create_memory_prompt.format(conversation=past_messages, memory_profile=formatted_memory))\n",
    "        structured_model = model.with_structured_output(UserProfile)\n",
    "        updated_memory = structured_model.invoke([formatted_system_message])\n",
    "        key = \"user_memory\"\n",
    "        store.put(namespace, key, { \"memory\": updated_memory })\n",
    "        return END\n",
    "    else:\n",
    "        return \"agent_executor\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Graph\n",
    "Now that we've defined our agent's memory, nodes, and edges, let's build and compile our graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from IPython.display import Image, display\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"supervisor\", supervisor)\n",
    "builder.add_node(\"agent_executor\", agent_executor)\n",
    "builder.add_node(\"replanner\", replanner)\n",
    "builder.add_node(\"human_input\", human_input)\n",
    "builder.add_edge(START, \"supervisor\")\n",
    "builder.add_edge(\"supervisor\", \"human_input\")\n",
    "builder.add_edge(\"human_input\", \"agent_executor\")\n",
    "builder.add_edge(\"agent_executor\", \"replanner\")\n",
    "builder.add_conditional_edges(\n",
    "    \"replanner\",\n",
    "    should_end,\n",
    "    [\"agent_executor\", END],\n",
    ")\n",
    "\n",
    "memory_saver = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory_saver, store=in_memory_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_input = { \"messages\": [HumanMessage(content=\"my customer ID is 1. what is my name? also, whats my most recent purchase? and what albums does the catalog have by U2\")] }\n",
    "thread = {\"configurable\": {\"thread_id\": \"5\", \"user_id\": \"50\"}}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "result = graph.invoke(initial_input, config = thread) \n",
    "print(\"Would you like to make any revisions to the plan?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume interruption \n",
    "interrupt_info = \"\"\n",
    "result = graph.invoke(Command(resume=interrupt_info), config = thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = \"50\"\n",
    "namespace = (\"memory_profile\", user_id)\n",
    "memory = in_memory_store.get(namespace, \"user_memory\").value\n",
    "\n",
    "saved_music_preferences = memory.get(\"memory\").music_preferences\n",
    "\n",
    "print(saved_music_preferences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give feedback to the plan this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_input = { \"messages\": [HumanMessage(content=\"my customer ID is 1. what is my name? also, whats my most recent purchase? and what albums does the catalog have by U2\")] }\n",
    "thread = {\"configurable\": {\"thread_id\": \"103\", \"user_id\": \"50\"}}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "result = graph.invoke(initial_input, config = thread) \n",
    "print(\"Would you like to make any revisions to the plan?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume interruption \n",
    "interrupt_info = \"i forgot to include this in my original request, but do you also have any songs by Amy Winehouse?\"\n",
    "result = graph.invoke(Command(resume=interrupt_info), config = thread)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
